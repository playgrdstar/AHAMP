{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Experiments re. extension of HAMP as AHAMP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "##\n",
    "import pickle\n",
    "\n",
    "import argparse\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import scipy.sparse as sp\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import f1_score\n",
    "import networkx as nx\n",
    "import dgl\n",
    "import dgl.function as fn\n",
    "import dgl.nn.pytorch as dglnn\n",
    "from dgl.nn.pytorch import edge_softmax\n",
    "# from models import HAMP as Model\n",
    "from models import PosVect, InterModalAttention\n",
    "# note: there may still be some variability\n",
    "torch.manual_seed(8)\n",
    "np.random.seed(8)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# device = 'cpu'\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mode = 'adaptive'\n",
    "task = 'screen_genre_class' \n",
    "# task = 'element_comp_class'\n",
    "\n",
    "# paths\n",
    "home_dir = Path(os.getcwd())\n",
    "version_dir = 'rico_n'\n",
    "main_data_dir = home_dir/'data'\n",
    "data_dir = home_dir/'data'/version_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data - features\n",
    "app2ui_edgelist = pd.read_hdf(data_dir/'app2ui_edgelist.h5', key='edgelist')\n",
    "ui2class_edgelist = pd.read_hdf(data_dir/'ui2class_edgelist.h5', key='edgelist')\n",
    "class2element_edgelist = pd.read_hdf(data_dir/'class2element_edgelist.h5', key='edgelist')\n",
    "element2element_edgelist = pd.read_hdf(data_dir/'element2element_edgelist.h5', key='edgelist')\n",
    "app_description_features_df = pd.read_hdf(data_dir/'app_description_features.h5', key='features')\n",
    "ui_image_features = pd.read_hdf(data_dir/'ui_image_features.h5', key='features')\n",
    "ui_pos_features_df = pd.read_hdf(data_dir/'ui_position_features.h5', key='features')\n",
    "class_name_features = pd.read_hdf(data_dir/'charngram_features.h5', key='features') \n",
    "element_spatial_features_df = pd.read_hdf(data_dir/'spatial_features.h5', key='features')\n",
    "element_image_features_df = pd.read_hdf(data_dir/'element_image_features.h5', key='features')\n",
    "# load labels\n",
    "comp_labels = pd.read_hdf(data_dir/'comp_labels.h5', key='labels')\n",
    "genre_labels = pd.read_hdf(data_dir/'genre_labels.h5', key='labels')\n",
    "\n",
    "# process edgelists\n",
    "e2e_num_edges = len(element2element_edgelist)\n",
    "e2e_adj_row = list(element2element_edgelist.target_element_encoded)\n",
    "e2e_adj_col = list(element2element_edgelist.source_element_encoded)\n",
    "e2e_num = len(element2element_edgelist.target_element_encoded.unique())\n",
    "e2e_adj = sp.csc_matrix((np.ones(e2e_num_edges), (e2e_adj_row, e2e_adj_col)), shape=(e2e_num, e2e_num))\n",
    "e2c_num_edges = len(class2element_edgelist)\n",
    "e2c_adj_row = list(class2element_edgelist.target_element_encoded)\n",
    "e2c_adj_col = list(class2element_edgelist.class_name_encoded)\n",
    "e2c_num_row = len(class2element_edgelist.target_element_encoded.unique())\n",
    "e2c_num_col = len(class2element_edgelist.class_name_encoded.unique())\n",
    "e2c_adj = sp.csc_matrix((np.ones(e2c_num_edges), (e2c_adj_row, e2c_adj_col)), shape=(e2c_num_row, e2c_num_col))\n",
    "u2c_num_edges = len(ui2class_edgelist)\n",
    "u2c_adj_row = list(ui2class_edgelist.ui_encoded)\n",
    "u2c_adj_col = list(ui2class_edgelist.class_name_encoded)\n",
    "u2c_num_row = len(ui2class_edgelist.ui_encoded.unique())\n",
    "u2c_num_col = len(ui2class_edgelist.class_name_encoded.unique())\n",
    "u2c_adj = sp.csc_matrix((np.ones(u2c_num_edges), (u2c_adj_row, u2c_adj_col)), shape=(u2c_num_row, u2c_num_col))\n",
    "a2u_num_edges = len(app2ui_edgelist)\n",
    "a2u_adj_row = list(app2ui_edgelist.app_encoded)\n",
    "a2u_adj_col = list(app2ui_edgelist.ui_encoded)\n",
    "a2u_num_row = len(app2ui_edgelist.app_encoded.unique())\n",
    "a2u_num_col = len(app2ui_edgelist.ui_encoded.unique())\n",
    "a2u_adj = sp.csc_matrix((np.ones(a2u_num_edges), (a2u_adj_row, a2u_adj_col)), shape=(a2u_num_row, a2u_num_col))\n",
    "# process features\n",
    "assert (app_description_features_df.app_encoded == app2ui_edgelist.app_encoded.unique()).all()\n",
    "app_desc_vectors = app_description_features_df.iloc[:,3:].values\n",
    "assert (ui_image_features.ui_encoded == app2ui_edgelist.ui_encoded.unique()).all()\n",
    "assert (ui_image_features.ui_encoded == ui2class_edgelist.ui_encoded.unique()).all()\n",
    "ui_image_vectors = ui_image_features.iloc[:,2:].values\n",
    "assert (class_name_features.class_name_encoded == ui2class_edgelist.class_name_encoded.unique()).all()\n",
    "assert (class_name_features.class_name_encoded == ui2class_edgelist.class_name_encoded.unique()).all()\n",
    "class_name_vectors = class_name_features.iloc[:,4:].values\n",
    "assert (element_spatial_features_df.target_encoded == class2element_edgelist.target_element_encoded.unique()).all()\n",
    "assert (element_spatial_features_df.target_encoded == element2element_edgelist.target_element_encoded.unique()).all()\n",
    "element_spatial_vectors = element_spatial_features_df.iloc[:,2:].values\n",
    "assert (element_image_features_df.target_element_encoded == class2element_edgelist.target_element_encoded.unique()).all()\n",
    "assert (element_image_features_df.target_element_encoded == element2element_edgelist.target_element_encoded.unique()).all()\n",
    "element_image_vectors = element_image_features_df.iloc[:,2:].values\n",
    "assert (ui_image_features.ui_encoded == ui_pos_features_df.ui_encoded.unique()).all()\n",
    "assert (ui_image_features.ui_encoded == ui_pos_features_df.ui_encoded.unique()).all()\n",
    "\n",
    "ui_pos_vectors = ui_pos_features_df.iloc[:,3:].values\n",
    "\n",
    "G = dgl.heterograph({\n",
    "        ('element', 'fwd', 'element') : e2e_adj.nonzero(), # nonzero is the edgelist\n",
    "        ('element', 'bkwd', 'element') : e2e_adj.transpose().nonzero(),\n",
    "        ('element', 'is', 'class') : e2c_adj.nonzero(),\n",
    "        ('class', 'of', 'element') : e2c_adj.transpose().nonzero(),\n",
    "        # ('class', 'selfloop', 'class') : c2c_adj.nonzero(), # two-hop if necc\n",
    "        ('ui', 'composed-of', 'class') : u2c_adj.nonzero(),\n",
    "        ('class', 'in', 'ui') : u2c_adj.transpose().nonzero(),\n",
    "        ('app', 'inc', 'ui') : a2u_adj.nonzero(),\n",
    "        ('ui', 'part-of', 'app') : a2u_adj.transpose().nonzero(),\n",
    "    }, num_nodes_dict = {'element': e2e_adj.shape[1], 'class': e2c_adj.shape[1], 'ui': a2u_adj.shape[1], 'app':a2u_adj.shape[0]})\n",
    "\n",
    "# look-up dicts\n",
    "node_dict = {}\n",
    "reverse_node_dict = {}\n",
    "edge_dict = {}\n",
    "for ntype in G.ntypes:\n",
    "    idx = len(node_dict)\n",
    "    node_dict[ntype] = idx # increment by 1 each iteration\n",
    "    reverse_node_dict[idx] = ntype\n",
    "for etype in G.etypes:\n",
    "    edge_dict[etype] = len(edge_dict)\n",
    "    # assign a list of same integer ids to the etype\n",
    "    G.edges[etype].data['id'] = torch.ones(G.number_of_edges(etype), dtype=torch.long) * edge_dict[etype] # attribute of edge with the id\n",
    "    \n",
    "node_feat = {'app':app_desc_vectors.shape[1], 'class':class_name_vectors.shape[1], 'element':element_image_vectors.shape[1], 'ui': ui_image_vectors.shape[1]}\n",
    "\n",
    "G.nodes['element'].data['comp_label'] = torch.tensor(comp_labels.comp_encoded.values)\n",
    "G.nodes['ui'].data['genre_label'] = torch.tensor(genre_labels.genre_encoded.values)\n",
    "G.nodes['element'].data['node_ft'] = torch.tensor(element_image_vectors)\n",
    "G.nodes['class'].data['node_ft'] = torch.tensor(class_name_vectors)\n",
    "G.nodes['ui'].data['node_ft'] = torch.tensor(ui_image_vectors)\n",
    "G.nodes['app'].data['node_ft'] = torch.tensor(app_desc_vectors)\n",
    "element_spatial_vectors[:,4] = element_spatial_vectors[:,4] + 3\n",
    "G.nodes['element'].data['pos'] = torch.FloatTensor(np.zeros((G.nodes('element').size()[0], 1)))\n",
    "G.nodes['class'].data['pos'] = torch.FloatTensor(np.zeros((G.nodes('class').size()[0], 1)))\n",
    "G.nodes['ui'].data['pos'] = torch.FloatTensor(ui_pos_vectors)\n",
    "G.nodes['app'].data['pos'] = torch.FloatTensor(np.zeros((G.nodes('app').size()[0], 1)))\n",
    "G.nodes['element'].data['depth'] = torch.FloatTensor(element_spatial_vectors[:,4]).unsqueeze(1)\n",
    "G.nodes['class'].data['depth'] = torch.FloatTensor(np.ones((G.nodes('class').size()[0], 1))*3)\n",
    "G.nodes['ui'].data['depth'] = torch.FloatTensor(np.ones((G.nodes('ui').size()[0], 1))*2)\n",
    "G.nodes['app'].data['depth'] = torch.FloatTensor(np.ones((G.nodes('app').size()[0], 1))*1)\n",
    "G.nodes['element'].data['bound1'] = torch.FloatTensor(element_spatial_vectors[:,0]).unsqueeze(1)\n",
    "G.nodes['class'].data['bound1'] = torch.FloatTensor(np.zeros((G.nodes('class').size()[0], 1)))\n",
    "G.nodes['ui'].data['bound1'] = torch.FloatTensor(np.zeros((G.nodes('ui').size()[0], 1)))\n",
    "G.nodes['app'].data['bound1'] = torch.FloatTensor(np.zeros((G.nodes('app').size()[0], 1)))\n",
    "G.nodes['element'].data['bound2'] = torch.FloatTensor(element_spatial_vectors[:,1]).unsqueeze(1)\n",
    "G.nodes['class'].data['bound2'] = torch.FloatTensor(np.zeros((G.nodes('class').size()[0], 1)))\n",
    "G.nodes['ui'].data['bound2'] = torch.FloatTensor(np.zeros((G.nodes('ui').size()[0], 1)))\n",
    "G.nodes['app'].data['bound2'] = torch.FloatTensor(np.zeros((G.nodes('app').size()[0], 1)))\n",
    "G.nodes['element'].data['bound3'] = torch.FloatTensor(element_spatial_vectors[:,2]).unsqueeze(1)\n",
    "G.nodes['class'].data['bound3'] = torch.FloatTensor(np.zeros((G.nodes('class').size()[0], 1)))\n",
    "G.nodes['ui'].data['bound3'] = torch.FloatTensor(np.zeros((G.nodes('ui').size()[0], 1)))\n",
    "G.nodes['app'].data['bound3'] = torch.FloatTensor(np.zeros((G.nodes('app').size()[0], 1)))\n",
    "G.nodes['element'].data['bound4'] = torch.FloatTensor(element_spatial_vectors[:,3]).unsqueeze(1)\n",
    "G.nodes['class'].data['bound4'] = torch.FloatTensor(np.zeros((G.nodes('class').size()[0], 1)))\n",
    "G.nodes['ui'].data['bound4'] = torch.FloatTensor(np.zeros((G.nodes('ui').size()[0], 1)))\n",
    "G.nodes['app'].data['bound4'] = torch.FloatTensor(np.zeros((G.nodes('app').size()[0], 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AHAMPLayer(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim,\n",
    "                 node_dict, edge_dict,\n",
    "                 num_heads, dropout = 0.3, use_layer_norm = True):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.input_dim = input_dim # input dimension after spatial-sequence-hierarichal vectorization, inter-modal and spatial-sequence-hierarichal attention and projection\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.output_dim = output_dim # output dimension of each layer\n",
    "\n",
    "        self.node_dict = node_dict # dictionaries contain the node-types and edge-tyeps\n",
    "        self.edge_dict = edge_dict\n",
    "\n",
    "        self.num_node_types = len(node_dict)\n",
    "        self.num_edge_types = len(edge_dict)\n",
    "        self.num_heads = num_heads \n",
    "        self.d_k = hidden_dim // num_heads \n",
    "        self.sqrt_dk = math.sqrt(self.d_k) \n",
    "\n",
    "        self.k_dense = nn.ModuleList()\n",
    "        self.q_dense = nn.ModuleList()\n",
    "        self.v_dense = nn.ModuleList()\n",
    "        self.fc = nn.ModuleList()\n",
    "        self.layer_norms = nn.ModuleList()\n",
    "        self.dense = nn.ModuleList()\n",
    "        self.use_layer_norm = use_layer_norm \n",
    "        self.canon_weights   = nn.Parameter(torch.ones(self.num_edge_types, self.num_heads))\n",
    "        self.att_weights = nn.Parameter(torch.Tensor(self.num_edge_types, self.num_heads, self.d_k, self.d_k)) \n",
    "        self.value_weights = nn.Parameter(torch.Tensor(self.num_edge_types, self.num_heads, self.d_k, self.d_k)) \n",
    "        self.res = nn.Parameter(torch.ones(self.num_node_types))\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        for t in range(self.num_node_types): \n",
    "            self.k_dense.append(nn.Linear(input_dim, hidden_dim))\n",
    "            self.q_dense.append(nn.Linear(input_dim, hidden_dim))\n",
    "            self.v_dense.append(nn.Linear(input_dim, hidden_dim))\n",
    "            self.fc.append(nn.Linear(hidden_dim, hidden_dim))\n",
    "            if use_layer_norm:\n",
    "                self.layer_norms.append(nn.LayerNorm(hidden_dim))\n",
    "            self.dense.append(nn.Linear(hidden_dim, output_dim))\n",
    "        \n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        for p in self.parameters():\n",
    "            if p.dim() > 1:\n",
    "                # nn.init.xavier_normal_(p, gain=0.001) \n",
    "                nn.init.xavier_uniform_(p)\n",
    "\n",
    "    def forward(self, G, h, edge_mask=None):\n",
    "\n",
    "        with G.local_scope(): \n",
    "            for srctype, etype, dsttype in G.canonical_etypes:\n",
    "                sub_graph = G[srctype, etype, dsttype]\n",
    "\n",
    "                k_dense = self.k_dense[self.node_dict[srctype]] # this retrieves the models, node_dict number corresponds to dictionary position in the ModuleList\n",
    "                v_dense = self.v_dense[self.node_dict[srctype]]\n",
    "                q_dense = self.q_dense[self.node_dict[dsttype]]\n",
    "\n",
    "                k = k_dense(h[srctype]).view(-1, self.num_heads, self.d_k) # source\n",
    "                v = v_dense(h[srctype]).view(-1, self.num_heads, self.d_k) # source\n",
    "                q = q_dense(h[dsttype]).view(-1, self.num_heads, self.d_k) # target\n",
    "\n",
    "                # extract id for the edge\n",
    "                e_id = self.edge_dict[etype]\n",
    "\n",
    "                att_weights = self.att_weights[e_id] \n",
    "                canon_weights = self.canon_weights[e_id]\n",
    "                value_weights = self.value_weights[e_id]\n",
    "\n",
    "                k = torch.einsum(\"bij,ijk->bik\", k, att_weights)\n",
    "                v = torch.einsum(\"bij,ijk->bik\", v, value_weights)\n",
    "\n",
    "                sub_graph.srcdata['k'] = k # src and dst different in different subgraphs\n",
    "                sub_graph.dstdata['q'] = q\n",
    "                sub_graph.srcdata['v'] = v\n",
    "\n",
    "                sub_graph.apply_edges(fn.v_dot_u('q', 'k', 't')) \n",
    "                attn_score = sub_graph.edata.pop('t').sum(-1) * canon_weights / self.sqrt_dk \n",
    "                attn_score = edge_softmax(sub_graph, attn_score, norm_by='dst') \n",
    "\n",
    "                num_edges = sub_graph.num_edges()\n",
    "                \n",
    "                if edge_mask is not None:\n",
    "                    if num_edges > 0:\n",
    "                        # print(edge_mask[srctype, etype, dsttype].shape)\n",
    "                        edge_mask_ = edge_mask[srctype, etype, dsttype].sigmoid()\n",
    "                        edge_mask_ = edge_mask_.unsqueeze(-1)\n",
    "                        attn_score_masked = torch.mul(attn_score, edge_mask_)\n",
    "                        attn_score = attn_score_masked\n",
    "\n",
    "                sub_graph.edata['t'] = attn_score.unsqueeze(-1) \n",
    "\n",
    "                edge_scores = attn_score_masked\n",
    "\n",
    "            G.multi_update_all({etype : (fn.u_mul_e('v', 't', 'm'), fn.sum('m', 't')) for etype in self.edge_dict}, cross_reducer = 'mean')\n",
    "            final_h = {}\n",
    "            for ntype in G.ntypes:\n",
    "                n_id = self.node_dict[ntype]\n",
    "                alpha = torch.sigmoid(self.res[n_id]) \n",
    "                t = G.nodes[ntype].data['t'].view(-1, self.hidden_dim) \n",
    "                h_prime = self.dropout(self.fc[n_id](t))\n",
    "                h_prime = h_prime * alpha + h[ntype] * (1-alpha)\n",
    "                if self.use_layer_norm:\n",
    "                    final_h[ntype] = self.dense[n_id](self.layer_norms[n_id](h_prime))\n",
    "                else:\n",
    "                    final_h[ntype] = self.dense[n_id](h_prime)\n",
    "                \n",
    "            return final_h\n",
    "\n",
    "class AHAMP(nn.Module):\n",
    "    def __init__(self, node_dict, edge_dict, reverse_node_dict, node_feat, input_dim, hidden_dim, output_dim, num_layers, num_heads, use_layer_norm = True, act = F.gelu):\n",
    "        super().__init__()\n",
    "        self.node_dict = node_dict\n",
    "        self.edge_dict = edge_dict\n",
    "        self.layer = nn.ModuleList()\n",
    "        self.input_dim = input_dim # not used for now in HAMP as it is directly computed from node_feat dict, but can be utilized if all feature dimensions same\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.act = act\n",
    "\n",
    "        self.projection  = nn.ModuleList()\n",
    "\n",
    "        self.pvect_pos  = nn.ModuleList()   \n",
    "        self.pvect_depth  = nn.ModuleList()  \n",
    "        self.pvect_bound1  = nn.ModuleList()  \n",
    "        self.pvect_bound2  = nn.ModuleList()  \n",
    "        self.pvect_bound3  = nn.ModuleList()\n",
    "        self.pvect_bound4  = nn.ModuleList()  \n",
    "\n",
    "        for t in range(len(node_dict)):\n",
    "            self.pvect_pos.append(PosVect(1, hidden_dim))\n",
    "            self.pvect_depth.append(PosVect(1, hidden_dim)) # depth corresponds to hierarchy information\n",
    "            self.pvect_bound1.append(PosVect(1, hidden_dim))\n",
    "            self.pvect_bound2.append(PosVect(1, hidden_dim))\n",
    "            self.pvect_bound3.append(PosVect(1, hidden_dim))\n",
    "            self.pvect_bound4.append(PosVect(1, hidden_dim))\n",
    "\n",
    "            #project down from different feature dimensions\n",
    "            in_dim = node_feat[reverse_node_dict[t]] # this is a lookup that allows us to assign different input dimension to each of the projection modules (depending on the modality of the feature)\n",
    "            self.projection.append(nn.Linear(in_dim, hidden_dim))\n",
    "\n",
    "        self.intermodal_attention = InterModalAttention(in_size=hidden_dim)\n",
    "\n",
    "        for _ in range(num_layers):\n",
    "            # input_dim here is hidden_dim as it has already been projected from the various node_feat to a common hidden_dim\n",
    "            self.layer.append(AHAMPLayer(hidden_dim, hidden_dim, hidden_dim, node_dict, edge_dict, num_heads, use_layer_norm = use_layer_norm))\n",
    "        \n",
    "        self.out = nn.Linear(hidden_dim, output_dim) \n",
    "        \n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        for p in self.parameters():\n",
    "            if p.dim() > 1:\n",
    "                # nn.init.xavier_normal_(p, gain=0.001) \n",
    "                nn.init.xavier_uniform_(p)\n",
    "\n",
    "    def forward(self, G, sel_node_type, feat_mask=None, edge_mask=None):\n",
    "        h = {}\n",
    "        pos_h = {}\n",
    "        depth_h = {}\n",
    "        bound1_h = {}\n",
    "        bound2_h = {}\n",
    "        bound3_h = {}\n",
    "        bound4_h = {}\n",
    "\n",
    "        if feat_mask is not None:\n",
    "            for ntype in G.ntypes:\n",
    "                n_id = self.node_dict[ntype]\n",
    "\n",
    "                masked_node_ft = torch.mul(G.nodes[ntype].data['node_ft'], feat_mask[ntype]['node_ft'].sigmoid())\n",
    "                h[ntype] = self.act(self.projection[n_id](masked_node_ft))\n",
    "\n",
    "                masked_pos = torch.mul(G.nodes[ntype].data['pos'], feat_mask[ntype]['pos'].sigmoid())\n",
    "                pos_h[ntype] = self.act(self.pvect_pos[n_id](masked_pos))\n",
    "\n",
    "                masked_depth = torch.mul(G.nodes[ntype].data['depth'], feat_mask[ntype]['depth'].sigmoid())\n",
    "                depth_h[ntype] = self.act(self.pvect_depth[n_id](masked_depth))\n",
    "\n",
    "                masked_bound1 = torch.mul(G.nodes[ntype].data['bound1'], feat_mask[ntype]['bound1'].sigmoid())\n",
    "                bound1_h[ntype] = self.act(self.pvect_bound1[n_id](masked_bound1))\n",
    "\n",
    "                masked_bound2 = torch.mul(G.nodes[ntype].data['bound2'], feat_mask[ntype]['bound2'].sigmoid())\n",
    "                bound2_h[ntype] = self.act(self.pvect_bound1[n_id](masked_bound2))\n",
    "\n",
    "                masked_bound3 = torch.mul(G.nodes[ntype].data['bound3'], feat_mask[ntype]['bound3'].sigmoid())\n",
    "                bound3_h[ntype] = self.act(self.pvect_bound1[n_id](masked_bound3))\n",
    "\n",
    "                masked_bound4 = torch.mul(G.nodes[ntype].data['bound4'], feat_mask[ntype]['bound4'].sigmoid())\n",
    "                bound4_h[ntype] = self.act(self.pvect_bound1[n_id](masked_bound4))\n",
    "\n",
    "                # attention\n",
    "                all_h = []\n",
    "                all_h.append(h[ntype])\n",
    "                all_h.append(pos_h[ntype])\n",
    "                all_h.append(depth_h[ntype])\n",
    "                all_h.append(bound1_h[ntype])\n",
    "                all_h.append(bound2_h[ntype])\n",
    "                all_h.append(bound3_h[ntype])\n",
    "                all_h.append(bound4_h[ntype])\n",
    "\n",
    "                all_h = torch.stack(all_h, dim=1) \n",
    "                att_h = self.intermodal_attention(all_h)  \n",
    "                h[ntype] = att_h\n",
    "\n",
    "        else: \n",
    "\n",
    "            for ntype in G.ntypes:\n",
    "                n_id = self.node_dict[ntype]\n",
    "                h[ntype] = self.act(self.projection[n_id](G.nodes[ntype].data['node_ft'])) \n",
    "\n",
    "                pos_h[ntype] = self.act(self.pvect_pos[n_id](G.nodes[ntype].data['pos']))\n",
    "                depth_h[ntype] = self.act(self.pvect_depth[n_id](G.nodes[ntype].data['depth']))\n",
    "                bound1_h[ntype] = self.act(self.pvect_bound1[n_id](G.nodes[ntype].data['bound1']))\n",
    "                bound2_h[ntype] = self.act(self.pvect_bound2[n_id](G.nodes[ntype].data['bound2']))\n",
    "                bound3_h[ntype] = self.act(self.pvect_bound3[n_id](G.nodes[ntype].data['bound3']))\n",
    "                bound4_h[ntype] = self.act(self.pvect_bound4[n_id](G.nodes[ntype].data['bound4']))\n",
    "\n",
    "                # attention\n",
    "                all_h = []\n",
    "                all_h.append(h[ntype])\n",
    "                all_h.append(pos_h[ntype])\n",
    "                all_h.append(depth_h[ntype])\n",
    "                all_h.append(bound1_h[ntype])\n",
    "                all_h.append(bound2_h[ntype])\n",
    "                all_h.append(bound3_h[ntype])\n",
    "                all_h.append(bound4_h[ntype])\n",
    "\n",
    "                all_h = torch.stack(all_h, dim=1) \n",
    "                att_h = self.intermodal_attention(all_h)  \n",
    "                h[ntype] = att_h\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            h = self.layer[i](G, h, edge_mask)\n",
    "\n",
    "        return self.out(h[sel_node_type]) # select node type to get representation of\n",
    "\n",
    "def init_masks(graph, device):\n",
    "    feat_mask = {}\n",
    "    feat_types = ['node_ft', 'pos', 'depth', 'bound1', 'bound2', 'bound3', 'bound4']\n",
    "    for ntype in graph.ntypes:\n",
    "        temp = {}\n",
    "        for type in feat_types:\n",
    "            num_nodes, feat_size = graph.nodes[ntype].data[type].size()\n",
    "            std = 0.1\n",
    "            temp[type] = nn.Parameter(torch.randn(1, feat_size, device=device) * std)\n",
    "            feat_mask[ntype] = temp\n",
    "\n",
    "    edge_mask = {}\n",
    "    for srctype, etype, dsttype in graph.canonical_etypes:\n",
    "        sub_graph = graph[srctype, etype, dsttype]\n",
    "        num_edges = sub_graph.num_edges()\n",
    "        num_nodes = sub_graph.num_nodes()\n",
    "        if num_edges > 0:\n",
    "            std = nn.init.calculate_gain('relu') * math.sqrt(2.0 / (2 * num_nodes))\n",
    "            edge_mask[srctype, etype, dsttype] = nn.Parameter(torch.randn(num_edges, device=device) * std)\n",
    "\n",
    "    return feat_mask, edge_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize model\n",
    "n_epoch = 3000\n",
    "input_dim = None\n",
    "hidden_dim = 32\n",
    "clip = 1.0\n",
    "max_lr=1e-3\n",
    "task = task\n",
    "eps = 1e-15\n",
    "alpha1=0.005\n",
    "alpha2=1.0\n",
    "beta1=1.0\n",
    "beta2=0.1\n",
    "\n",
    "if task == 'screen_genre_class':\n",
    "    selected_element = 'ui'\n",
    "    num_classes = len(G.nodes['ui'].data['genre_label'].unique())\n",
    "    labels = G.nodes['ui'].data['genre_label']\n",
    "    pid = u2c_adj.tocoo().row\n",
    "    \n",
    "elif task == 'element_comp_class':\n",
    "    selected_element = 'element'\n",
    "    num_classes = len(G.nodes['element'].data['comp_label'].unique())\n",
    "    labels = G.nodes['element'].data['comp_label']\n",
    "    pid = e2e_adj.tocoo().row\n",
    "\n",
    "genre_labels_ = G.nodes['ui'].data['genre_label']\n",
    "comp_labels_ = G.nodes['element'].data['comp_label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_mask, edge_mask = init_masks(G, device)\n",
    "\n",
    "# generate train/val/test split\n",
    "train = int(0.6*len(pid))\n",
    "valid = int(0.8*len(pid))\n",
    "shuffle = np.random.permutation(pid)\n",
    "train_idx = torch.tensor(shuffle[0:train]).long()\n",
    "val_idx = torch.tensor(shuffle[train:valid]).long()\n",
    "test_idx = torch.tensor(shuffle[valid:]).long()\n",
    "\n",
    "model = AHAMP(node_dict, edge_dict, reverse_node_dict, node_feat, \n",
    "            input_dim=input_dim,\n",
    "            hidden_dim=hidden_dim,\n",
    "            output_dim=num_classes,\n",
    "            num_layers=2,\n",
    "            num_heads=2,\n",
    "            use_layer_norm=True).to(device)\n",
    "\n",
    "# optimizer = torch.optim.AdamW(model.parameters())\n",
    "\n",
    "params = list(model.parameters()) + [edge_mask[k] for k in edge_mask.keys()]\n",
    "optimizer = torch.optim.AdamW(params)\n",
    "\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, total_steps=n_epoch, max_lr = max_lr)\n",
    "G = G.to(device) \n",
    "\n",
    "# train\n",
    "best_val_acc = 0\n",
    "best_test_acc = 0\n",
    "best_micro_f1 = 0\n",
    "best_macro_f1 = 0\n",
    "train_step = 0\n",
    "best_epoch = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main training loop\n",
    "for epoch in tqdm(np.arange(n_epoch)+1):\n",
    "    model.train()\n",
    "    logits = model(G, selected_element, edge_mask=edge_mask)\n",
    "    # The loss is computed only for labeled nodes.\n",
    "    loss = F.cross_entropy(logits[train_idx], labels[train_idx].to(device))\n",
    "\n",
    "    for srctype, etype, dsttype in G.canonical_etypes:\n",
    "        num_edges = G[srctype, etype, dsttype].num_edges()\n",
    "        if num_edges > 0:\n",
    "            e_mask_s = edge_mask[srctype, etype, dsttype].sigmoid()\n",
    "            # edge_mask = edge_mask.sigmoid()\n",
    "            # Edge mask sparsity regularization\n",
    "            loss = loss + alpha1 * torch.sum(e_mask_s)\n",
    "            # Edge mask entropy regularization\n",
    "            ent = - e_mask_s * torch.log(e_mask_s + eps) - \\\n",
    "                (1 - e_mask_s) * torch.log(1 - e_mask_s + eps)\n",
    "            loss = loss + alpha2 * ent.mean()\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "    optimizer.step()\n",
    "    train_step += 1\n",
    "    scheduler.step()\n",
    "    \n",
    "    if epoch % 5 == 0:\n",
    "        model.eval()\n",
    "        logits = model(G, selected_element, edge_mask=edge_mask)\n",
    "        pred   = logits.argmax(1).cpu()\n",
    "        train_acc = (pred[train_idx] == labels[train_idx]).float().mean()\n",
    "        val_acc   = (pred[val_idx]   == labels[val_idx]).float().mean()\n",
    "        test_acc  = (pred[test_idx]  == labels[test_idx]).float().mean()\n",
    "\n",
    "        test_micro_f1 = f1_score(labels[test_idx].detach().cpu().numpy(), pred[test_idx].numpy(), average='micro')\n",
    "        test_macro_f1 = f1_score(labels[test_idx].detach().cpu().numpy(), pred[test_idx].numpy(), average='macro')\n",
    "\n",
    "        if best_val_acc < val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            best_test_acc = test_acc\n",
    "            if (best_micro_f1 < test_micro_f1) & (best_macro_f1 < test_macro_f1):\n",
    "                best_micro_f1 = test_micro_f1\n",
    "                best_macro_f1 = test_macro_f1\n",
    "                best_epoch = epoch\n",
    "\n",
    "print('='*100)\n",
    "print(f'Test - Best - Micro F1: {best_micro_f1} | Macro F1: {best_macro_f1} | Best epoch: {best_epoch}')\n",
    "print('='*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7f595395498f6d16acf1ffacc65ce0bc3cd09f5095ebd42de87aada63741f9f8"
  },
  "kernelspec": {
   "display_name": "Python 3.9.10 ('py39')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
